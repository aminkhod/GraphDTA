{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgo8cLB7Ayqg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Et0csGA4of",
        "outputId": "eaeaccd0-b516-4af3-c1f4-7c83c8b95f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCJG-aZCA4lg",
        "outputId": "c21d6e15-4042-4852-e84c-a45333cf4a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'drive': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqBB0ST7A4i_",
        "outputId": "4929cc12-7b6f-4f85-e02b-c43bab79b66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/GraphDTA-master'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/GraphDTA-master'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xsOYH5eqLNn"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b_OMfravpyX"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYc3W8E4A4fw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747d8a4e-5a1a-4c9f-f5a7-10a4a5767904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=baa13a3ab3a7a3d48f0919c339f86e1426e34ad4778720430e2948ccb4689fe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I6H3tkEBMmf",
        "outputId": "674952e6-6f8a-44d3-ff7f-ae40287ff5cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision numpy pandas scikit-learn tqdm matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SqX64FABMhW",
        "outputId": "2c20a443-b326-479b-f594-d0071a92ed5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwFW6ZtGBMdo"
      },
      "outputs": [],
      "source": [
        "!pip install dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbTx4r7DqzrX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys, os\n",
        "from random import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from models.gat import GATNet\n",
        "from models.gat_gcn import GAT_GATNet\n",
        "from models.gcn import GCNNet\n",
        "from models.ginconv import GINConvNet\n",
        "from utils import *\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SglIrZyXq5C4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#import dataloader as dataloader\n",
        "# training function at each epoch\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
        "    model.train()\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
        "                                            batch_idx * len(data.x),\n",
        "                                            len(train_loader.dataset),\n",
        "                                            100. * batch_idx / len(train_loader),\n",
        "                                            loss.item()))\n",
        "\n",
        "def predicting(model, device, loader):\n",
        "    model.eval()\n",
        "    total_preds = torch.Tensor()\n",
        "    total_labels = torch.Tensor()\n",
        "    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
        "            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
        "    return total_labels.numpy().flatten(),total_preds.numpy().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Tensorflow"
      ],
      "metadata": {
        "id": "dvwYgB5W0fK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Scikit-learn\n",
        "!pip install Matplotlib Seaborn\n",
        "!pip install subword_nmt periodictable tensorflow_addons\n"
      ],
      "metadata": {
        "id": "VkCLVf2Q0kBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dEmAFv7_0jxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import json\n",
        "import itertools\n",
        "import periodictable as pt\n",
        "import re\n",
        "from itertools import chain\n",
        "from operator import itemgetter\n",
        "from subword_nmt.apply_bpe import BPE\n",
        "import codecs"
      ],
      "metadata": {
        "id": "HTSE-Gwnz86t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PilPO5CR0fG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create data.**py**"
      ],
      "metadata": {
        "id": "sRHRI1bUz94t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12WbCMugsD45"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json, pickle\n",
        "from collections import OrderedDict\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import MolFromSmiles\n",
        "import networkx as nx\n",
        "from utils import *\n",
        "import sys, os\n",
        "from random import shuffle\n",
        "from models.gat import GATNet\n",
        "from models.gat_gcn import GAT_GATNet\n",
        "from models.gcn import GCNNet\n",
        "from models.ginconv import GINConvNet\n",
        "from utils import *\n",
        "\n",
        "\n",
        "def atom_features(atom):\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
        "                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n",
        "                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n",
        "                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n",
        "                                           'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    [atom.GetIsAromatic()])\n",
        "\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "\n",
        "def smile_to_graph(smile):\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "\n",
        "    c_size = mol.GetNumAtoms()\n",
        "\n",
        "    features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        feature = atom_features(atom)\n",
        "        features.append(feature / sum(feature))\n",
        "\n",
        "    edges = []\n",
        "    for bond in mol.GetBonds():\n",
        "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
        "    g = nx.Graph(edges).to_directed()\n",
        "    edge_index = []\n",
        "    for e1, e2 in g.edges:\n",
        "        edge_index.append([e1, e2])\n",
        "\n",
        "    return c_size, features, edge_index\n",
        "\n",
        "    drugs = []\n",
        "    compound_iso_smiles = []\n",
        "    for dt_name in ['kiba', 'davis']:\n",
        "        opts = ['train', 'test']\n",
        "        for opt in opts:\n",
        "            df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n",
        "            compound_iso_smiles += list(df['compound_iso_smiles'])\n",
        "    compound_iso_smiles = set(compound_iso_smiles)\n",
        "    smile_graph = {}\n",
        "    for smile in compound_iso_smiles:\n",
        "        g = smile_to_graph(smile)\n",
        "        smile_graph[smile] = g\n",
        "\n",
        "\n",
        "#####from DTITR#####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####FROM MOLTRANS####\n",
        "\n",
        "class BIN_Interaction_Flat(nn.Sequential):\n",
        "    '''\n",
        "        Interaction Network with 2D interaction map\n",
        "    '''\n",
        "\n",
        "    def __init__(self, **config):\n",
        "        super(BIN_Interaction_Flat, self).__init__()\n",
        "        # self.max_d = config['max_drug_seq']\n",
        "        self.max_p = config['max_protein_seq']\n",
        "        self.emb_size = config['emb_size']\n",
        "        self.dropout_rate = config['dropout_rate']\n",
        "\n",
        "    def forward(self, d, p, d_mask, p_mask):\n",
        "        # ex_d_mask = d_mask.unsqueeze(1).unsqueeze(2)\n",
        "        ex_p_mask = p_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # ex_d_mask = (1.0 - ex_d_mask) * -10000.0\n",
        "        ex_p_mask = (1.0 - ex_p_mask) * -10000.0\n",
        "\n",
        "        # d_emb = self.demb(d) # batch_size x seq_length x embed_size\n",
        "        p_emb = self.pemb(p)\n",
        "\n",
        "        # set output_all_encoded_layers be false, to obtain the last layer hidden states only...\n",
        "\n",
        "        # d_encoded_layers = self.d_encoder(d_emb.float(), ex_d_mask.float())\n",
        "        # print(d_encoded_layers.shape)\n",
        "        #p_encoded_layers = self.p_encoder(p_emb.float(), ex_p_mask.float())\n",
        "        # print(p_encoded_layers.shape)\n",
        "\n",
        "        # repeat to have the same tensor size for aggregation\n",
        "        # d_aug = torch.unsqueeze(d_encoded_layers, 2).repeat(1, 1, self.max_p, 1) # repeat along protein size\n",
        "        #sp_aug = torch.unsqueeze(p_encoded_layers, 1).repeat(1, self.max_d, 1, 1)  # repeat along drug size\n",
        "\n",
        "\n",
        "\n",
        "        ##\n",
        "        # specialized embedding with positional one\n",
        "        # self.demb = Embeddings(self.input_dim_drug, self.emb_size, self.max_d, self.dropout_rate)\n",
        "        self.pemb = Embeddings(self.input_dim_target, self.emb_size, self.max_p, self.dropout_rate)\n",
        "\n",
        "        # self.d_encoder = Encoder_MultipleLayers(self.n_layer, self.hidden_size, self.intermediate_size, self.num_attention_heads, self.attention_probs_dropout_prob, self.hidden_dropout_prob)\n",
        "        #self.p_encoder = Encoder_MultipleLayers(self.n_layer, self.hidden_size, self.intermediate_size,\n",
        "                                                ##self.hidden_dropout_prob)\n",
        "\n",
        "        self.icnn = nn.Conv2d(1, 3, 3, padding=0)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.flatten_dim, 512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Linear(512, 64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # output layer\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from protein/target, position embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, hidden_size, max_position_size, dropout_rate):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(max_position_size, hidden_size)\n",
        "\n",
        "        #self.LayerNorm = LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA-TTZ19BeV3",
        "outputId": "0891d02a-7ff7-4a7f-9704-4df6489845f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  0.0005\n",
            "Epochs:  2\n",
            "\n",
            "running on  GAT_GATNet_davis\n",
            "Pre-processed data found: data/processed/davis_train.pt, loading ...\n",
            "Pre-processed data found: data/processed/davis_test.pt, loading ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 25046 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch: 1 [0/25046 (0%)]\tLoss: 0.107014\n",
            "Train epoch: 1 [329680/25046 (41%)]\tLoss: 0.135768\n",
            "Train epoch: 1 [657720/25046 (82%)]\tLoss: 0.093029\n",
            "Make prediction for 5010 samples...\n",
            "rmse improved at epoch  1 ; best_mse,best_ci: 0.24873847 0.8773788627588337 GAT_GATNet davis\n",
            "Training on 25046 samples...\n",
            "Train epoch: 2 [0/25046 (0%)]\tLoss: 0.072308\n",
            "Train epoch: 2 [331360/25046 (41%)]\tLoss: 0.098862\n",
            "Train epoch: 2 [659080/25046 (82%)]\tLoss: 0.069238\n",
            "Make prediction for 5010 samples...\n",
            "0.2679761 No improvement since epoch  1 ; best_mse,best_ci: 0.24873847 0.8773788627588337 GAT_GATNet davis\n"
          ]
        }
      ],
      "source": [
        "dataset = 'davis'\n",
        "#kiba\n",
        "modeling = [GAT_GATNet]\n",
        "model_st = modeling[0].__name__\n",
        "\n",
        "cuda_name = \"cuda:0\"\n",
        "# if len(sys.argv)>3:\n",
        "#     cuda_name = \"cuda:\" + str(int(sys.argv[3]))\n",
        "# print('cuda_name:', cuda_name)\n",
        "\n",
        "TRAIN_BATCH_SIZE = 512\n",
        "TEST_BATCH_SIZE = 512\n",
        "LR = 0.0005\n",
        "LOG_INTERVAL = 20\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "print('Learning rate: ', LR)\n",
        "print('Epochs: ', NUM_EPOCHS)\n",
        "\n",
        "# Main program: iterate over different datasets\n",
        "# for dataset in dataset:\n",
        "print('\\nrunning on ', model_st + '_' + dataset )\n",
        "processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n",
        "processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n",
        "if ((not os.path.isfile(processed_data_file_train)) or \\\n",
        " (not os.path.isfile(processed_data_file_test))):\n",
        "    print('please run create_data.py to prepare data in pytorch format!')\n",
        "else:\n",
        "    train_data = TestbedDataset(root='data', dataset=dataset+'_train')\n",
        "    test_data = TestbedDataset(root='data', dataset=dataset+'_test')\n",
        "\n",
        "    # make data PyTorch mini-batch processing ready\n",
        "    train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# training the model\n",
        "    device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
        "    #model = modeling\n",
        "    for modeling in [GAT_GATNet]:\n",
        "      model = modeling().to(device)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    best_mse = 0.24929152\n",
        "    best_ci =  0.8811023101087032\n",
        "    best_epoch = 853\n",
        "    model_file_name = 'model_' + model_st + '_' + dataset +  '.model'\n",
        "    result_file_name = 'result_' + model_st + '_' + dataset +  '.csv'\n",
        "    model.load_state_dict(torch.\\\n",
        "    load('/content/drive/MyDrive/GraphDTA-master/model_GAT_GATNet_davis.model',\\\n",
        "         map_location=torch.device('cpu'))\n",
        "    )\n",
        "     #map_location=torch.device('cpu'))) # load the state_dict on CPU\n",
        "    # model.train()\n",
        "\n",
        "    mse_list = []\n",
        "    ci_list = []\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      torch.cuda.empty_cache()\n",
        "      train(model, device, train_loader, optimizer, epoch+1)\n",
        "      G,P = predicting(model, device, test_loader)\n",
        "      ret = [rmse(G,P),mse(G,P),pearson(G,P),spearman(G,P),ci(G,P)]\n",
        "\n",
        "\n",
        "      mse_list.append(ret[1])\n",
        "      ci_list.append(ret[-1])\n",
        "      if ret[1]<best_mse:\n",
        "          torch.save(model.state_dict(), \\\n",
        "                '/content/drive/MyDrive/GraphDTA-master/model_GAT_GATNet_davis.model')\n",
        "          with open(result_file_name,'w') as f:\n",
        "              f.write(','.join(map(str,ret)))\n",
        "          best_epoch = epoch+1\n",
        "          best_mse = ret[1]\n",
        "          best_ci = ret[-1]\n",
        "\n",
        "\n",
        "          print('rmse improved at epoch ', best_epoch, '; best_mse,best_ci:',\\\n",
        "                best_mse,best_ci,model_st,dataset)\n",
        "\n",
        "      else:\n",
        "          print(ret[1],'No improvement since epoch ', best_epoch,\\\n",
        "                '; best_mse,best_ci:', best_mse,best_ci,model_st,dataset)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJN2dLKR8CGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nf4J2xrJ61o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}