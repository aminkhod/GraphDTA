{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rgo8cLB7Ayqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24dd011a-6d95-4a91-be33-813faa60bb3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0_Et0csGA4of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bef369c-7c6b-4fd1-b3d6-e7d3d22f1745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCJG-aZCA4lg",
        "outputId": "e01c92dc-6475-497e-96c6-b84899b53903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive  Othercomputers\n"
          ]
        }
      ],
      "source": [
        "!ls drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqBB0ST7A4i_",
        "outputId": "0f80e6e9-bedd-437e-9db2-753a92413a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GraphDTA-master-2\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/GraphDTA-master-2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xsOYH5eqLNn"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3b_OMfravpyX"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYc3W8E4A4fw"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I6H3tkEBMmf"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision numpy pandas scikit-learn tqdm matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SqX64FABMhW"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwFW6ZtGBMdo"
      },
      "outputs": [],
      "source": [
        "!pip install dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbTx4r7DqzrX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys, os\n",
        "from random import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from models.gat import GATNet\n",
        "from models.gat_gcn import GAT_GATNet\n",
        "from models.gcn import GCNNet\n",
        "from models.ginconv import GINConvNet\n",
        "from utils import *\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvwYgB5W0fK_"
      },
      "outputs": [],
      "source": [
        "#!pip install Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkCLVf2Q0kBT"
      },
      "outputs": [],
      "source": [
        "!pip install Scikit-learn\n",
        "!pip install Matplotlib Seaborn\n",
        "!pip install subword_nmt periodictable tensorflow_addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HTSE-Gwnz86t"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "import glob\n",
        "import json\n",
        "import itertools\n",
        "import periodictable as pt\n",
        "import re\n",
        "from itertools import chain\n",
        "from operator import itemgetter\n",
        "from subword_nmt.apply_bpe import BPE\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.python.compiler.tensorrt import trt_convert as trt"
      ],
      "metadata": {
        "id": "EURbWVh8nb2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"Tensorflow version: \", tf.version.VERSION)"
      ],
      "metadata": {
        "id": "ZPFgObgZnrzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PilPO5CR0fG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2973f6c-e9af-41dc-9a2b-75b602dcb1ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert data from DeepDTA for  davis1\n",
            "\n",
            "dataset: davis1\n",
            "train_fold: 25046\n",
            "test_fold: 5010\n",
            "len(set(drugs)),len(set(prots)): 68 379\n",
            "preparing  davis1_train.pt in pytorch format!\n",
            "Pre-processed data data/processed/davis1_train.pt not found, doing pre-processing...\n",
            "Converting SMILES to graph: 1/25046\n",
            "/content/drive/MyDrive/GraphDTA-master-2/utils.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  GCNData = DATA.Data(x=torch.Tensor(features),\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/GraphDTA-master-2/create_data.py\", line 292, in <module>\n",
            "    train_data = TestbedDataset(root='data', dataset=dataset + '_train', xd=train_drugs, xt=train_prots, y=train_Y,\n",
            "  File \"/content/drive/MyDrive/GraphDTA-master-2/utils.py\", line 28, in __init__\n",
            "    self.process(xd, xt, y,smile_graph)\n",
            "  File \"/content/drive/MyDrive/GraphDTA-master-2/utils.py\", line 71, in process\n",
            "    GCNData.target = torch.LongTensor([target])\n",
            "ValueError: too many dimensions 'str'\n"
          ]
        }
      ],
      "source": [
        "!python '/content/drive/MyDrive/GraphDTA-master-2/create_data.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **create_data.py**"
      ],
      "metadata": {
        "id": "vRLqCfcop6GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json,pickle\n",
        "from collections import OrderedDict\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import MolFromSmiles\n",
        "import networkx as nx\n",
        "from utils import *\n",
        "import torch as nn\n",
        "import glob\n",
        "import json\n",
        "#import periodictable as pt\n",
        "import re\n",
        "from itertools import chain\n",
        "from operator import itemgetter\n",
        "from subword_nmt.apply_bpe import BPE\n",
        "import codecs\n",
        "#from GraphDTA#######\n",
        "\n",
        "def atom_features(atom):\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
        "                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n",
        "                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n",
        "                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n",
        "                                           'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    [atom.GetIsAromatic()])\n",
        "\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def smile_to_graph(smile):\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "\n",
        "    c_size = mol.GetNumAtoms()\n",
        "\n",
        "    features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        feature = atom_features(atom)\n",
        "        features.append(feature / sum(feature))\n",
        "\n",
        "    edges = []\n",
        "    for bond in mol.GetBonds():\n",
        "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
        "    g = nx.Graph(edges).to_directed()\n",
        "    edge_index = []\n",
        "    for e1, e2 in g.edges:\n",
        "        edge_index.append([e1, e2])\n",
        "\n",
        "    return c_size, features, edge_index\n",
        "    drugs = []\n",
        "    compound_iso_smiles = []\n",
        "    for dt_name in ['kiba', 'davis']:\n",
        "        opts = ['train', 'test']\n",
        "        for opt in opts:\n",
        "            df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n",
        "            compound_iso_smiles += list(df['compound_iso_smiles'])\n",
        "    compound_iso_smiles = set(compound_iso_smiles)\n",
        "    smile_graph = {}\n",
        "    for smile in compound_iso_smiles:\n",
        "        g = smile_to_graph(smile)\n",
        "        smile_graph[smile] = g\n",
        "############## from DTITR ##############################################################\n",
        "class dataset_builder():\n",
        "    def __init__(self, data_path, **kwargs):\n",
        "        super(dataset_builder, self).__init__(**kwargs)\n",
        "        self.data_path = data_path\n",
        "\n",
        "    def get_data(self):\n",
        "        dataset = pd.read_csv(self.data_path['data'], sep=',', memory_map=True)\n",
        "        prot_dictionary = json.load(open(self.data_path['prot_dic']))\n",
        "        #smiles_dictionary = json.load(open(self.data_path['smiles_dic']))\n",
        "        clusters = []\n",
        "        bpe_codes_prot = ''\n",
        "        bpe_codes_map_prot = ''\n",
        "\n",
        "        for i in self.data_path['clusters']:\n",
        "            if 'test' in i:\n",
        "                clusters.append(('test', pd.read_csv(i, header=None)))\n",
        "            else:\n",
        "                clusters.append(('train', pd.read_csv(i, header=None)))\n",
        "\n",
        "        if self.data_path['prot_bpe'] != '':\n",
        "            bpe_codes_prot = codecs.open(self.data_path['prot_bpe'][0])\n",
        "            bpe_codes_map_prot = pd.read_csv(self.data_path['prot_bpe'][1])\n",
        "\n",
        "        return (dataset, prot_dictionary, clusters, bpe_codes_prot, bpe_codes_map_prot)\n",
        "\n",
        "    def data_conversion(self, data, dictionary, max_len):\n",
        "        keys = list(i for i in dictionary.keys() if len(i) > 1)\n",
        "\n",
        "        if len(keys) == 0:\n",
        "            data = pd.DataFrame([list(i) for i in data])\n",
        "\n",
        "        else:\n",
        "            char_list = []\n",
        "            for i in data:\n",
        "                positions = []\n",
        "                for j in keys:\n",
        "                    positions.extend([(k.start(), k.end() - k.start()) for k in re.finditer(j, i)])\n",
        "\n",
        "                positions = sorted(positions, key=itemgetter(0))\n",
        "\n",
        "                if len(positions) == 0:\n",
        "                    char_list.append(list(i))\n",
        "\n",
        "                else:\n",
        "                    new_list = []\n",
        "                    j = 0\n",
        "                    positions_start = [k[0] for k in positions]\n",
        "                    positions_len = [k[1] for k in positions]\n",
        "\n",
        "                    while j < len(i):\n",
        "                        if j in positions_start:\n",
        "                            new_list.append(str(i[j] + i[j + positions_len[positions_start.index(j)] - 1]))\n",
        "                            j = j + positions_len[positions_start.index(j)]\n",
        "                        else:\n",
        "                            new_list.append(i[j])\n",
        "                            j = j + 1\n",
        "                    char_list.append(new_list)\n",
        "\n",
        "            data = pd.DataFrame(char_list)\n",
        "\n",
        "        data.replace(dictionary, inplace=True)\n",
        "\n",
        "        data = data.fillna(0)\n",
        "        if len(data.iloc[0, :]) == max_len:\n",
        "            return data\n",
        "        else:\n",
        "            zeros_array = np.zeros(shape=(len(data.iloc[:, 0]), max_len - len(data.iloc[0, :])))\n",
        "            data = pd.concat((data, pd.DataFrame(zeros_array)), axis=1)\n",
        "            return data\n",
        "\n",
        "    def encoding_bpe(self, data, codes, codes_map, max_len):\n",
        "        bpe = BPE(codes, merges=-1, separator='')\n",
        "        idx2word = codes_map['index'].values\n",
        "        words2idx = dict(zip(idx2word, range(0, len(idx2word))))\n",
        "\n",
        "        vectors = []\n",
        "\n",
        "        for i in data:\n",
        "            t1 = bpe.process_line(i).split()  # split\n",
        "            try:\n",
        "                i1 = np.asarray([words2idx[j] + 1 for j in t1])  # index\n",
        "            except:\n",
        "                i1 = np.array([0])\n",
        "\n",
        "            l = len(i1)\n",
        "\n",
        "            if l < max_len:\n",
        "                k = np.pad(i1, (0, max_len - l), 'constant', constant_values=0)\n",
        "            else:\n",
        "                k = i1[:max_len]\n",
        "            vectors.append(k[None, :])\n",
        "\n",
        "        return torch.int32(torch.cat(vectors, dim=0))\n",
        "\n",
        "\n",
        "\n",
        "    def transform_dataset(self, prot_bpe_enc_opt,\n",
        "                          protein_column,\n",
        "                          kd_column, bpe_prot_max_len, prot_max_len):\n",
        "\n",
        "      if prot_bpe_enc_opt == True:\n",
        "\n",
        "         protein_data = self.encoding_bpe(self.get_data()[0][protein_column], self.get_data()[4],\n",
        "                                     self.get_data()[5], bpe_prot_max_len)\n",
        "\n",
        "\n",
        "      else:\n",
        "\n",
        "         protein_data = torch.tensor(self.data_conversion(self.get_data()[0][protein_column],\n",
        "                                                     self.get_data()[1], prot_max_len).astype('int32'),\n",
        "                                dtype=torch.int32)\n",
        "\n",
        "\n",
        "      kd_values = self.get_data()[0][kd_column].astype('float32')\n",
        "\n",
        "      #print(protein_data)\n",
        "      return protein_data, kd_values\n",
        "\n",
        "\n",
        "all_prots = []\n",
        "datasets = ['davis1']\n",
        "for dataset in datasets:\n",
        "    print('convert data from DeepDTA for ', dataset)\n",
        "    fpath = 'data/' + dataset + '/'\n",
        "    train_fold = json.load(open(fpath + \"folds/train_fold_setting1.txt\"))\n",
        "    train_fold = [ee for e in train_fold for ee in e]\n",
        "    valid_fold = json.load(open(fpath +\"folds/test_fold_setting1.txt\"))\n",
        "    ligands = json.load(open(fpath + \"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
        "    proteins = json.load(open(fpath + \"proteins.txt\"), object_pairs_hook=OrderedDict)\n",
        "    affinity = pickle.load(open(fpath + \"Y\", \"rb\"), encoding='latin1')\n",
        "    drugs = []\n",
        "    prots = []\n",
        "    for d in ligands.keys():\n",
        "        lg = Chem.MolToSmiles(Chem.MolFromSmiles(ligands[d]), isomericSmiles=True)\n",
        "        drugs.append(lg)\n",
        "    for t in proteins.keys():\n",
        "        prots.append(proteins[t])\n",
        "    if dataset == 'davis':\n",
        "        affinity = [-np.log10(y / 1e9) for y in affinity]\n",
        "    affinity = np.asarray(affinity)\n",
        "    opts = ['train', 'test']\n",
        "    for opt in opts:\n",
        "        rows, cols = np.where(np.isnan(affinity) == False)\n",
        "        if opt == 'train':\n",
        "            rows, cols = rows[train_fold], cols[train_fold]\n",
        "        elif opt == 'test':\n",
        "            rows, cols = rows[valid_fold], cols[valid_fold]\n",
        "        with open('data/' + dataset + '_' + opt + '.csv', 'w') as f:\n",
        "            f.write('compound_iso_smiles,target_sequence,affinity\\n')\n",
        "            for pair_ind in range(len(rows)):\n",
        "                ls = []\n",
        "                ls += [drugs[rows[pair_ind]]]\n",
        "                ls += [prots[cols[pair_ind]]]\n",
        "                ls += [affinity[rows[pair_ind], cols[pair_ind]]]\n",
        "                f.write(','.join(map(str, ls)) + '\\n')\n",
        "    print('\\ndataset:', dataset)\n",
        "    print('train_fold:', len(train_fold))\n",
        "    print('test_fold:', len(valid_fold))\n",
        "    print('len(set(drugs)),len(set(prots)):', len(set(drugs)), len(set(prots)))\n",
        "    all_prots += list(set(prots))\n",
        "\n",
        "compound_iso_smiles = []\n",
        "for dt_name in ['kiba','davis']:\n",
        "    opts = ['train','test']\n",
        "    for opt in opts:\n",
        "        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n",
        "        compound_iso_smiles += list( df['compound_iso_smiles'] )\n",
        "compound_iso_smiles = set(compound_iso_smiles)\n",
        "smile_graph = {}\n",
        "for smile in compound_iso_smiles:\n",
        "    g = smile_to_graph(smile)\n",
        "    smile_graph[smile] = g\n",
        "\n",
        "class dataset_builder:\n",
        "    def transform_dataset(self, prot_bpe_enc_opt, protein_column, kd_column, bpe_prot_max_len, protein_sequence):\n",
        "      return protein_sequence\n",
        "\n",
        "    def get_data(self):\n",
        "        dataset = pd.read_csv(self.data_path['data'], sep=',', memory_map=True)\n",
        "        prot_dictionary = json.load(open(self.data_path['prot_dic']))\n",
        "        # smiles_dictionary = json.load(open(self.data_path['smiles_dic']))\n",
        "        clusters = []\n",
        "        bpe_codes_prot = ''\n",
        "        bpe_codes_map_prot = ''\n",
        "        return (dataset, prot_dictionary, clusters, bpe_codes_prot, bpe_codes_map_prot)\n",
        "\n",
        "\n",
        "data_builder = dataset_builder()\n",
        "#train_Y = list(data_builder.get_data()[0]['affinity'])\n",
        "prot_bpe_enc_opt = True\n",
        "protein_column = 'target_sequence'\n",
        "kd_column = 'affinity'\n",
        "bpe_prot_max_len = 1000\n",
        "\n",
        "datasets = ['davis1']\n",
        "# convert to PyTorch data format\n",
        "for dataset in datasets:\n",
        "    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n",
        "    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n",
        "    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n",
        "        df = pd.read_csv('data/' + dataset + '_train.csv')\n",
        "        train_drugs, train_prots,  train_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n",
        "        XT = [data_builder.transform_dataset(prot_bpe_enc_opt, protein_column, kd_column, bpe_prot_max_len, t) for t in train_prots]\n",
        "        train_drugs, train_prots,  train_Y = np.asarray(train_drugs), np.asarray(XT), np.asarray(train_Y)\n",
        "        df = pd.read_csv('data/' + dataset + '_test.csv')\n",
        "        test_drugs, test_prots,  test_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n",
        "        XT = [data_builder.transform_dataset(prot_bpe_enc_opt, protein_column, kd_column, bpe_prot_max_len, t) for t in test_prots]\n",
        "        test_drugs, test_prots,  test_Y = np.asarray(test_drugs), np.asarray(XT), np.asarray(test_Y)\n",
        "\n",
        "        # make data PyTorch Geometric ready\n",
        "\n",
        "        # Convert target values to floats\n",
        "        train_Y = [float(y) for y in train_Y]\n",
        "\n",
        "        print('preparing ', dataset + '_train.pt in pytorch format!')\n",
        "        train_data = TestbedDataset(root='data', dataset=dataset + '_train', xd=train_drugs, xt=train_prots, y=train_Y,\n",
        "                                    smile_graph=smile_graph)\n",
        "        print('preparing ', dataset + '_test.pt in pytorch format!')\n",
        "        test_data = TestbedDataset(root='data', dataset=dataset+'_test', xd=test_drugs, xt=test_prots, y=test_Y,smile_graph=smile_graph)\n",
        "        print(processed_data_file_train, ' and ', processed_data_file_test, ' have been created')\n",
        "    else:\n",
        "        print(processed_data_file_train, ' and ', processed_data_file_test, ' are already created')\n",
        "\n"
      ],
      "metadata": {
        "id": "3jjXeiW0laxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "fe3e1239-5b7d-4df8-dd7e-78453c97b4e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert data from DeepDTA for  davis1\n",
            "\n",
            "dataset: davis1\n",
            "train_fold: 25046\n",
            "test_fold: 5010\n",
            "len(set(drugs)),len(set(prots)): 68 379\n",
            "preparing  davis1_train.pt in pytorch format!\n",
            "Pre-processed data data/processed/davis1_train.pt not found, doing pre-processing...\n",
            "Converting SMILES to graph: 1/25046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/GraphDTA-master-2/utils.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  GCNData = DATA.Data(x=torch.Tensor(features),\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-87f3905a3d6f>\u001b[0m in \u001b[0;36m<cell line: 273>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preparing '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_train.pt in pytorch format!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         train_data = TestbedDataset(root='data', dataset=dataset + '_train', xd=train_drugs, xt=train_prots, y=train_Y,\n\u001b[0m\u001b[1;32m    293\u001b[0m                                     smile_graph=smile_graph)\n\u001b[1;32m    294\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preparing '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_test.pt in pytorch format!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/GraphDTA-master-2/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, dataset, xd, xt, y, transform, pre_transform, smile_graph)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pre-processed data {} not found, doing pre-processing...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmile_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/GraphDTA-master-2/utils.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, xd, xt, y, smile_graph)\u001b[0m\n\u001b[1;32m     69\u001b[0m                                 \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                 y=torch.FloatTensor([labels]))\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mGCNData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mGCNData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'c_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# append graph, label and target sequence to data list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **training.py**"
      ],
      "metadata": {
        "id": "jV_PHTpjqUk2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DurZ4dAtqOBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nf4J2xrJ61o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}