{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgo8cLB7Ayqg",
    "outputId": "e5df475b-7274-404e-a65e-31f7632392b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_Et0csGA4of",
    "outputId": "61ea9c57-d67d-4e95-cdb4-1539a87e2691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCJG-aZCA4lg",
    "outputId": "92099f8d-3498-432a-8862-532e3af5ac26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDrive  Othercomputers\n"
     ]
    }
   ],
   "source": [
    "!ls drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqBB0ST7A4i_",
    "outputId": "b144ee4f-d3a8-4896-c1c4-6621d5c32cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/GraphDTA-master-2\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/MyDrive/GraphDTA-master-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xsOYH5eqLNn"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3b_OMfravpyX"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYc3W8E4A4fw"
   },
   "outputs": [],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I6H3tkEBMmf"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision numpy pandas scikit-learn tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SqX64FABMhW"
   },
   "outputs": [],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwFW6ZtGBMdo"
   },
   "outputs": [],
   "source": [
    "!pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DbTx4r7DqzrX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.gat import GATNet\n",
    "from models.gat_gcn import GAT_GATNet\n",
    "from models.gcn import GCNNet\n",
    "from models.ginconv import GINConvNet\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvwYgB5W0fK_"
   },
   "outputs": [],
   "source": [
    "!pip install Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkCLVf2Q0kBT"
   },
   "outputs": [],
   "source": [
    "!pip install Scikit-learn\n",
    "!pip install Matplotlib Seaborn\n",
    "!pip install subword_nmt periodictable tensorflow_addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HTSE-Gwnz86t"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import json\n",
    "import itertools\n",
    "import periodictable as pt\n",
    "import re\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EURbWVh8nb2m"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPFgObgZnrzK",
    "outputId": "ecfbc414-bfe0-4b0d-d73b-7e690f5d8ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \", tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoC3pOlPil79",
    "outputId": "929d09cf-c4c5-4da0-dfc6-2c4a97af459a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-27 19:21:05.056750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 \"/content/drive/MyDrive/GraphDTA-master-2/dtitr_model.py\" --option Train --num_epochs 500 --batch_dim 32 --prot_transformer_depth 3 --smiles_transformer_depth 3 --cross_block_depth 1 --prot_transformer_heads 4 --smiles_transformer_heads 4 --cross_block_heads 4 --prot_parameter_sharing '' --prot_dim_k 0 --prot_ff_dim 512 --smiles_ff_dim 512 --d_model 128 --dropout_rate 0.1 --dense_atv_fun gelu --out_mlp_depth 3 --out_mlp_hdim 512 512 512 --optimizer_fn radam 1e-04 0.9 0.999 1e-08 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ik8ot93TuXRw",
    "outputId": "1fa46ff4-309a-49e1-fda6-4edd35778d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(sys.version)\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpYh8PxwruyE",
    "outputId": "4341444a-e1d3-4240-f6e2-c975e6286675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorRT\n",
      "  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: tensorRT\n",
      "  Building wheel for tensorRT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorRT: filename=tensorrt-8.6.1.post1-py2.py3-none-any.whl size=17281 sha256=7e603be61a639535869cb744480a69d10d373a462cf18b7121129a69af91eba3\n",
      "  Stored in directory: /root/.cache/pip/wheels/f4/c8/0e/b79b08e45752491b9acfdbd69e8a609e8b2ed7640dda5a3e59\n",
      "Successfully built tensorRT\n",
      "Installing collected packages: tensorRT\n",
      "Successfully installed tensorRT-8.6.1.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dLyvQgK8p79O"
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PilPO5CR0fG5",
    "outputId": "fc719489-51e7-4f69-93a5-36b26c43702c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-27 19:25:39.407141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!python '/content/drive/MyDrive/GraphDTA-master-2/create_data.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRLqCfcop6GP"
   },
   "source": [
    "# **create_data.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jjXeiW0laxH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "import networkx as nx\n",
    "from utils import *\n",
    "import sys, os\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.gat import GATNet\n",
    "from models.gat_gcn import GAT_GATNet\n",
    "from models.gcn import GCNNet\n",
    "from models.ginconv import GINConvNet\n",
    "\n",
    "from torch.utils import data\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import json\n",
    "import periodictable as pt\n",
    "import re\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "import codecs\n",
    "\n",
    "\n",
    "#from GraphDTA#######\n",
    "\n",
    "def atom_features(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n",
    "                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n",
    "                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n",
    "                                           'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "                    [atom.GetIsAromatic()])\n",
    "\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "\n",
    "    c_size = mol.GetNumAtoms()\n",
    "\n",
    "    features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feature = atom_features(atom)\n",
    "        features.append(feature / sum(feature))\n",
    "\n",
    "    edges = []\n",
    "    for bond in mol.GetBonds():\n",
    "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "    g = nx.Graph(edges).to_directed()\n",
    "    edge_index = []\n",
    "    for e1, e2 in g.edges:\n",
    "        edge_index.append([e1, e2])\n",
    "\n",
    "    return c_size, features, edge_index\n",
    "    drugs = []\n",
    "    compound_iso_smiles = []\n",
    "    for dt_name in ['kiba', 'davis']:\n",
    "        opts = ['train', 'test']\n",
    "        for opt in opts:\n",
    "            df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n",
    "            compound_iso_smiles += list(df['compound_iso_smiles'])\n",
    "    compound_iso_smiles = set(compound_iso_smiles)\n",
    "    smile_graph = {}\n",
    "    for smile in compound_iso_smiles:\n",
    "        g = smile_to_graph(smile)\n",
    "        smile_graph[smile] = g\n",
    "\n",
    "############## from DTITR ##############################################################\n",
    "\n",
    "\n",
    "class dataset_builder():\n",
    "    def __init__(self, data_path, **kwargs):\n",
    "        super(dataset_builder, self).__init__(**kwargs)\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def get_data(self):\n",
    "        dataset = pd.read_csv(self.data_path['data'], sep=',', memory_map=True)\n",
    "        prot_dictionary = json.load(open(self.data_path['prot_dic']))\n",
    "        #smiles_dictionary = json.load(open(self.data_path['smiles_dic']))\n",
    "        clusters = []\n",
    "        bpe_codes_prot = ''\n",
    "        bpe_codes_map_prot = ''\n",
    "        #bpe_codes_smiles = ''\n",
    "        #bpe_codes_map_smiles = ''\n",
    "\n",
    "        for i in self.data_path['clusters']:\n",
    "            if 'test' in i:\n",
    "                clusters.append(('test', pd.read_csv(i, header=None)))\n",
    "            else:\n",
    "                clusters.append(('train', pd.read_csv(i, header=None)))\n",
    "\n",
    "        if self.data_path['prot_bpe'] != '':\n",
    "            bpe_codes_prot = codecs.open(self.data_path['prot_bpe'][0])\n",
    "            bpe_codes_map_prot = pd.read_csv(self.data_path['prot_bpe'][1])\n",
    "\n",
    "        #if self.data_path['smiles_bpe'] != '':\n",
    "            #bpe_codes_smiles = codecs.open(self.data_path['smiles_bpe'][0])\n",
    "            #bpe_codes_map_smiles = pd.read_csv(self.data_path['smiles_bpe'][1])\n",
    "\n",
    "        return (dataset, prot_dictionary, clusters, bpe_codes_prot, bpe_codes_map_prot)\n",
    "\n",
    "    def data_conversion(self, data, dictionary, max_len):\n",
    "        keys = list(i for i in dictionary.keys() if len(i) > 1)\n",
    "\n",
    "        if len(keys) == 0:\n",
    "            data = pd.DataFrame([list(i) for i in data])\n",
    "\n",
    "        else:\n",
    "            char_list = []\n",
    "            for i in data:\n",
    "                positions = []\n",
    "                for j in keys:\n",
    "                    positions.extend([(k.start(), k.end() - k.start()) for k in re.finditer(j, i)])\n",
    "\n",
    "                positions = sorted(positions, key=itemgetter(0))\n",
    "\n",
    "                if len(positions) == 0:\n",
    "                    char_list.append(list(i))\n",
    "\n",
    "                else:\n",
    "                    new_list = []\n",
    "                    j = 0\n",
    "                    positions_start = [k[0] for k in positions]\n",
    "                    positions_len = [k[1] for k in positions]\n",
    "\n",
    "                    while j < len(i):\n",
    "                        if j in positions_start:\n",
    "                            new_list.append(str(i[j] + i[j + positions_len[positions_start.index(j)] - 1]))\n",
    "                            j = j + positions_len[positions_start.index(j)]\n",
    "                        else:\n",
    "                            new_list.append(i[j])\n",
    "                            j = j + 1\n",
    "                    char_list.append(new_list)\n",
    "\n",
    "            data = pd.DataFrame(char_list)\n",
    "\n",
    "        data.replace(dictionary, inplace=True)\n",
    "\n",
    "        data = data.fillna(0)\n",
    "        if len(data.iloc[0, :]) == max_len:\n",
    "            return data\n",
    "        else:\n",
    "            zeros_array = np.zeros(shape=(len(data.iloc[:, 0]), max_len - len(data.iloc[0, :])))\n",
    "            data = pd.concat((data, pd.DataFrame(zeros_array)), axis=1)\n",
    "            return data\n",
    "\n",
    "    def encoding_bpe(self, data, codes, codes_map, max_len):\n",
    "        bpe = BPE(codes, merges=-1, separator='')\n",
    "        idx2word = codes_map['index'].values\n",
    "        words2idx = dict(zip(idx2word, range(0, len(idx2word))))\n",
    "\n",
    "        vectors = []\n",
    "\n",
    "        for i in data:\n",
    "            t1 = bpe.process_line(i).split()  # split\n",
    "            try:\n",
    "                i1 = np.asarray([words2idx[j] + 1 for j in t1])  # index\n",
    "            except:\n",
    "                i1 = np.array([0])\n",
    "\n",
    "            l = len(i1)\n",
    "\n",
    "            if l < max_len:\n",
    "                k = np.pad(i1, (0, max_len - l), 'constant', constant_values=0)\n",
    "            else:\n",
    "                k = i1[:max_len]\n",
    "            vectors.append(k[None, :])\n",
    "\n",
    "        return tf.cast(tf.concat(vectors, axis=0), dtype=tf.int32)\n",
    "\n",
    "    def transform_dataset(self, prot_bpe_enc_opt,\n",
    "                          protein_column,\n",
    "                          kd_column, bpe_prot_max_len, prot_max_len):\n",
    "\n",
    "        if prot_bpe_enc_opt == True:\n",
    "\n",
    "            protein_data = self.encoding_bpe(self.get_data()[0][protein_column], self.get_data()[4],\n",
    "                                             self.get_data()[5], bpe_prot_max_len)\n",
    "\n",
    "        else:\n",
    "\n",
    "            protein_data = tf.convert_to_tensor(self.data_conversion(self.get_data()[0][protein_column],\n",
    "                                                                     self.get_data()[1], prot_max_len).astype('int32'),\n",
    "                                                dtype=tf.int32)\n",
    "\n",
    "\n",
    "        kd_values = self.get_data()[0][kd_column].astype('float32')\n",
    "\n",
    "        return protein_data, kd_values\n",
    "\n",
    "#######EMBEDDING LAYER FRO DTITR#####\n",
    "\n",
    "\n",
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Embedding Layer: generates a learned embedding to every token with a fixed size\n",
    "\n",
    "    Args:\n",
    "    - voc_size [int]: number of unique tokens\n",
    "    - d_model [int]: embedding dimension\n",
    "    - dropout_rate [float]: % of dropout\n",
    "    - positional_enc [boolean]: positional encoding option: if true adds a positional embedding\n",
    "    to the output of the embedding layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, voc_size, d_model, dropout_rate, positional_enc=True, **kwargs):\n",
    "        super(EmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.voc_size = voc_size\n",
    "        self.d_model = d_model\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.positional_enc = positional_enc\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.emb_layer = tf.keras.layers.Embedding(self.voc_size, self.d_model)\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "\n",
    "    def position_embedding(self, max_len):\n",
    "        \"\"\"\n",
    "        Positional Embedding: Adds info about the position of each token using sin and cosine functions\n",
    "\n",
    "        Args:\n",
    "        - max_len [int]: number of input tokens (length)\n",
    "\n",
    "        Shape:\n",
    "        - Outputs:\n",
    "        - pos_enc: (L,E) where L is the input sequence length, E is the embedding dimension\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        angle = tf.range(self.d_model, dtype=tf.float32)\n",
    "        angle = 10000 ** (2 * (angle / self.d_model))\n",
    "\n",
    "        angle = tf.expand_dims(tf.range(max_len, dtype=tf.float32), 1) / angle\n",
    "\n",
    "        # for i in range(angle.shape[0]):\n",
    "        #     for j in range(angle.shape[1]):\n",
    "        #         if j % 2 == 0 :\n",
    "        #             angle = tf.tensor_scatter_nd_update(angle,[[i,j]],[tf.math.sin(angle[i,j])])\n",
    "        #         else :\n",
    "        #             angle = tf.tensor_scatter_nd_update(angle,[[i,j]],[tf.math.cos(angle[i,j])])\n",
    "\n",
    "        # return tf.cast(angle,dtype=tf.float32)\n",
    "\n",
    "        values = tf.stack([tf.math.sin(angle[:, 0::2]), tf.math.cos(angle[:, 1::2])], axis=2)\n",
    "\n",
    "        pos_enc = tf.reshape(values, shape=[tf.shape(values)[0], -1])\n",
    "\n",
    "        return tf.cast(pos_enc, dtype=tf.float32)\n",
    "\n",
    "    def call(self, sequences):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "        - sequences: input sequences\n",
    "\n",
    "        Shape:\n",
    "        - Inputs:\n",
    "        - sequences: (B,L) where B is the batch size, L is the sequence length\n",
    "        - Outputs:\n",
    "        - output: (B,L,E) where B is the batch size, L is the input sequence length,\n",
    "                        E is the embedding dimension\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        max_len = sequences.shape[1]\n",
    "\n",
    "        output = self.emb_layer(sequences) * tf.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
    "\n",
    "        if self.positional_enc:  # Add Positional Info\n",
    "            output = output + self.position_embedding(max_len)\n",
    "            output = self.dropout_layer(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(EmbeddingLayer, self).get_config()\n",
    "        config.update({\n",
    "            'voc_size': self.voc_size,\n",
    "            'd_model': self.d_model,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'positional_enc': self.positional_enc})\n",
    "\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV_PHTpjqUk2"
   },
   "source": [
    "# **training.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DurZ4dAtqOBa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txyrAA_RmF_M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.gat import GATNet\n",
    "from models.gat_gcn import GAT_GATNet\n",
    "from models.gcn import GCNNet\n",
    "from models.ginconv import GINConvNet\n",
    "from argparse import ArgumentParser\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: NelsonRCM\n",
    "\"\"\"\n",
    "\n",
    "from transformer_encoder import *\n",
    "from create_data import *\n",
    "from layers_utils import *\n",
    "from output_block import *\n",
    "from create_data import *\n",
    "import itertools\n",
    "import tensorflow_addons as tfa\n",
    "from argument_parser import *\n",
    "import gc\n",
    "from plot_utils import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def build_dtitr_model(FLAGS, prot_trans_depth,\n",
    "                      prot_trans_heads,\n",
    "                      prot_parameter_sharing, prot_dim_k,\n",
    "                      prot_d_ff, d_model, dropout_rate, dense_atv_fun,\n",
    "                      out_mlp_depth, out_mlp_units, optimizer_fn):\n",
    "    \"\"\"\n",
    "    Function to build the DTITR Model\n",
    "\n",
    "    Args:\n",
    "    - FLAGS: arguments object\n",
    "    - prot_trans_depth [int]: number of protein transformer-encoders\n",
    "    - smiles_trans_depth [int]: number of SMILES transformer-encoders\n",
    "    - cross_attn_depth [int]: number of cross-attention transformer-encoders\n",
    "    - prot_trans_heads [int]: number of heads for the protein self-attention mha\n",
    "    - smiles_trans_heads [int]: number of heads for the smiles self-attention mha\n",
    "    - cross_attn_heads [int]: number of heads for the cross-attention mha\n",
    "    - prot_parameter_sharing [str]: protein parameter sharing option in the case of Linear MHA\n",
    "    - prot_dim_k [int]: protein Linear MHA projection dimension\n",
    "    - prot_d_ff [int]: hidden numbers for the first dense layer of the FFN in the case of the proteins\n",
    "    - smiles_d_ff [int]: hidden numbers for the first dense layer of the FFN in the case of the smiles\n",
    "    - d_model [int]: embedding dim\n",
    "    - dropout_rate [float]: % of dropout\n",
    "    - dense_atv_fun: dense layers activation function\n",
    "    - out_mlp_depth [int]: FCNN number of layers\n",
    "    - out_mlp_units [list of ints]: hidden neurons for each one of the dense layers of the FCNN\n",
    "    - optimizer_fn: optimizer function\n",
    "\n",
    "    Outputs:\n",
    "    - dtitr_model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if FLAGS.bpe_option[0]:\n",
    "        prot_input = tf.keras.Input(shape=(FLAGS.protein_bpe_len + 1,), dtype=tf.int32, name='protein_input')\n",
    "        prot_mask = attn_pad_mask()(prot_input)\n",
    "        encode_prot = EmbeddingLayer(FLAGS.protein_dict_bpe_len + 2, d_model,  # FLAGS.protein_bpe_len+1,\n",
    "                                     dropout_rate, FLAGS.pos_enc_option)(prot_input)\n",
    "\n",
    "    else:\n",
    "        prot_input = tf.keras.Input(shape=(FLAGS.protein_len + 1,), dtype=tf.int32, name='protein_input')\n",
    "        prot_mask = attn_pad_mask()(prot_input)\n",
    "        encode_prot = EmbeddingLayer(FLAGS.protein_dict_len + 2, d_model,  # FLAGS.protein_len+1,\n",
    "                                     dropout_rate, FLAGS.pos_enc_option)(prot_input)\n",
    "\n",
    "    encode_prot, _ = Encoder(d_model, prot_trans_depth, prot_trans_heads, prot_d_ff, dense_atv_fun,\n",
    "                             dropout_rate, prot_dim_k, prot_parameter_sharing,\n",
    "                             FLAGS.prot_full_attn,\n",
    "                             FLAGS.return_intermediate, name='encoder_prot')(encode_prot, prot_mask)\n",
    "\n",
    "    out = OutputMLP(out_mlp_depth, out_mlp_units, dense_atv_fun,\n",
    "                    FLAGS.output_atv_fun, dropout_rate, name='output_block')(cross_prot_smiles)\n",
    "\n",
    "\n",
    "    dtitr_model = tf.keras.Model(inputs=[prot_input, smiles_input], outputs=out, name='dtitr')\n",
    "\n",
    "    dtitr_model.compile(optimizer=optimizer_fn, loss=FLAGS.loss_function,\n",
    "                        metrics=[tf.keras.metrics.RootMeanSquaredError(), c_index])\n",
    "\n",
    "    # tf.keras.utils.plot_model(dtitr_model, to_file='./dtitr.png', dpi=600)\n",
    "\n",
    "    return dtitr_model\n",
    "\n",
    "\n",
    "def chemogenomic_folds_grid_search(FLAGS, data, folds, model_function):\n",
    "    \"\"\"\n",
    "    Grid Search function\n",
    "\n",
    "    Args:\n",
    "    - FLAGS: arguments object\n",
    "    - data: [protein data, smiles data, kd values]\n",
    "    - folds: [fold_1,fold_2,...,fold_n] fold_n: indices for the fold x\n",
    "    - model_function: function that creates the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    epochs_set = FLAGS.num_epochs\n",
    "    batch_set = FLAGS.batch_dim\n",
    "    p_enc_depth = FLAGS.prot_transformer_depth\n",
    "    s_enc_depth = FLAGS.smiles_transformer_depth\n",
    "    cross_depth = FLAGS.cross_block_depth\n",
    "    p_enc_heads = FLAGS.prot_transformer_heads\n",
    "    s_enc_heads = FLAGS.smiles_transformer_heads\n",
    "    cross_heads = FLAGS.cross_block_heads\n",
    "    prot_param_sharing_set = FLAGS.prot_parameter_sharing\n",
    "    prot_dim_k_set = FLAGS.prot_dim_k\n",
    "    prot_dff = FLAGS.prot_ff_dim\n",
    "    smiles_dff = FLAGS.smiles_ff_dim\n",
    "    d_model_set = FLAGS.d_model\n",
    "    drop_rate_set = FLAGS.dropout_rate\n",
    "    dense_act_set = FLAGS.dense_atv_fun\n",
    "    out_block_depth = FLAGS.out_mlp_depth\n",
    "    out_block_units_set = FLAGS.out_mlp_hdim\n",
    "    opt_set = FLAGS.optimizer_fn\n",
    "\n",
    "    logging(\"--------------------Grid Search-------------------\", FLAGS)\n",
    "\n",
    "    for params in itertools.product(epochs_set, batch_set, p_enc_depth, s_enc_depth, cross_depth, p_enc_heads,\n",
    "                                    s_enc_heads, cross_heads, prot_param_sharing_set, prot_dim_k_set,\n",
    "                                    prot_dff, smiles_dff, d_model_set, drop_rate_set,\n",
    "                                    dense_act_set, out_block_depth, out_block_units_set, opt_set):\n",
    "\n",
    "        p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14, p15, p16, p17, p18 = params\n",
    "\n",
    "        results = []\n",
    "\n",
    "        if p18[0] == 'radam':\n",
    "            p18 = tfa.optimizers.RectifiedAdam(learning_rate=float(p18[1]), beta_1=float(p18[2]),\n",
    "                                               beta_2=float(p18[3]), epsilon=float(p18[4]),\n",
    "                                               weight_decay=float(p18[5]))\n",
    "        elif p18[0] == 'adam':\n",
    "            p18 = tf.keras.optimizers.Adam(learning_rate=float(p18[1]), beta_1=float(p18[2]),\n",
    "                                           beta_2=float(p18[3]), epsilon=float(p18[4]))\n",
    "\n",
    "        elif p18[0] == 'adamw':\n",
    "            p18 = tfa.optimizers.AdamW(learning_rate=float(p18[1]), beta_1=float(p18[2]),\n",
    "                                       beta_2=float(p18[3]), epsilon=float(p18[4]),\n",
    "                                       weight_decay=float(p18[5]))\n",
    "\n",
    "        for fold_idx in range(len(folds)):\n",
    "            index_train = list(itertools.chain.from_iterable([folds[i] for i in range(len(folds)) if i != fold_idx]))\n",
    "\n",
    "            index_val = folds[fold_idx]\n",
    "\n",
    "            data_train = [tf.gather(i, index_train) for i in data]\n",
    "\n",
    "            data_val = [tf.gather(i, index_val) for i in data]\n",
    "\n",
    "            es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  min_delta=0.001, patience=50, mode='min',\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "            # mc = tf.keras.callbacks.ModelCheckpoint(filepath = FLAGS.checkpoint_path+'/'+str(fold_idx)+'/',\n",
    "            #                                         monitor = 'val_root_mean_squared_error',\n",
    "            #                                save_best_only=True, save_weights_only = True, mode = 'min')\n",
    "\n",
    "            grid_model = model_function(FLAGS, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14, p15, p16, p17, p18)\n",
    "\n",
    "            grid_model.fit(x=[data_train[0], data_train[1]], y=data_train[2],\n",
    "                           batch_size=p2, epochs=p1,\n",
    "                           verbose=2, callbacks=[es],\n",
    "                           validation_data=([data_val[0], data_val[1]], data_val[2]))\n",
    "\n",
    "            mse, rmse, ci = grid_model.evaluate([data_val[0], data_val[1]], data_val[2])\n",
    "            results.append((mse, rmse, ci))\n",
    "\n",
    "            logging((\"Epochs = %d,  Batch = %d, P Enc Depth = %d, S Enc Depth = %d, Cross Depth = %d, P Heads = %d, \" +\n",
    "                     \"S Heads = %d, Cross Heads = %d, Prot P Sharing = %s, Prot Dim K = %d, \" +\n",
    "                     \"P DFF = %d, S DFF = %d, D Model = %d, DropR = %0.2f, \" +\n",
    "                     \"Dense AF = %s, Out Depth = %d, Out Units = %s, Optimizer = %s, \" +\n",
    "                     \"Fold = %d, MSE = %0.3f, RMSE = %0.3f, CI = %0.3f\") %\n",
    "                    (p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14,\n",
    "                     p15, p16, p17, p18.get_config(), fold_idx, mse, rmse, ci), FLAGS)\n",
    "\n",
    "            del data_train\n",
    "            del data_val\n",
    "            del grid_model\n",
    "            gc.collect()\n",
    "\n",
    "        logging(\"Mean Folds - \" + (\" MSE = %0.3f, RMSE = %0.3f, CI = %0.3f\" % (np.mean(results, axis=0)[0],\n",
    "                                                                               np.mean(results, axis=0)[1],\n",
    "                                                                               np.mean(results, axis=0)[2])), FLAGS)\n",
    "\n",
    "\n",
    "def run_grid_search(FLAGS):\n",
    "    \"\"\"\n",
    "    Run Grid Search function\n",
    "\n",
    "    Args:\n",
    "    - FLAGS: arguments object\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model_function = build_dtitr_model\n",
    "    protein_data, smiles_data, kd_values = dataset_builder(FLAGS.data_path).transform_dataset(FLAGS.bpe_option[0],\n",
    "                                                                                              FLAGS.bpe_option[1],\n",
    "                                                                                              'Sequence',\n",
    "                                                                                              'SMILES',\n",
    "                                                                                              'Kd',\n",
    "                                                                                              FLAGS.protein_bpe_len,\n",
    "                                                                                              FLAGS.protein_len,\n",
    "                                                                                              FLAGS.smiles_bpe_len,\n",
    "                                                                                              FLAGS.smiles_len)\n",
    "    if FLAGS.bpe_option[0]:\n",
    "        protein_data = add_reg_token(protein_data, FLAGS.protein_dict_bpe_len)\n",
    "    else:\n",
    "        protein_data = add_reg_token(protein_data, FLAGS.protein_dict_len)\n",
    "\n",
    "    #if FLAGS.bpe_option[1]:\n",
    "        #smiles_data = add_reg_token(smiles_data, FLAGS.smiles_dict_bpe_len)\n",
    "    #else:\n",
    "        #smiles_data = add_reg_token(smiles_data, FLAGS.smiles_dict_len)\n",
    "\n",
    "        # kd_values = tf.expand_dims(kd_values,axis=1)\n",
    "\n",
    "    _, _, _, clusters, _, _, _, _ = dataset_builder(FLAGS.data_path).get_data()\n",
    "\n",
    "    clusters = [list(clusters[i][1].iloc[:, 0]) for i in range(len(clusters)) if clusters[i][0] != 'test']\n",
    "\n",
    "    chemogenomic_folds_grid_search(FLAGS, [protein_data, smiles_data, kd_values], clusters, model_function)\n",
    "\n",
    "\n",
    "def run_train_model(FLAGS):\n",
    "    \"\"\"\n",
    "    Run Train function\n",
    "\n",
    "    Args:\n",
    "    - FLAGS: arguments object\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    protein_data,  kd_values = dataset_builder(FLAGS.data_path).transform_dataset(FLAGS.bpe_option[0],\n",
    "                                                                                              FLAGS.bpe_option[1],\n",
    "                                                                                              'Sequence',\n",
    "                                                                                              'SMILES',\n",
    "                                                                                              'Kd',\n",
    "                                                                                              FLAGS.protein_bpe_len,\n",
    "                                                                                              FLAGS.protein_len,\n",
    "                                                                                              FLAGS.smiles_bpe_len,\n",
    "                                                                                              FLAGS.smiles_len)\n",
    "\n",
    "    if FLAGS.bpe_option[0] == True:\n",
    "        protein_data = add_reg_token(protein_data, FLAGS.protein_dict_bpe_len)\n",
    "    else:\n",
    "        protein_data = add_reg_token(protein_data, FLAGS.protein_dict_len)\n",
    "\n",
    "    #if FLAGS.bpe_option[1] == True:\n",
    "        #smiles_data = add_reg_token(smiles_data, FLAGS.smiles_dict_bpe_len)\n",
    "    #else:\n",
    "        #smiles_data = add_reg_token(smiles_data, FLAGS.smiles_dict_len)\n",
    "\n",
    "        # kd_values = tf.expand_dims(kd_values,axis=1)\n",
    "\n",
    "    _, _, _, clusters, _, _, _, _ = dataset_builder(FLAGS.data_path).get_data()\n",
    "\n",
    "    train_idx = pd.concat([i.iloc[:, 0] for t, i in clusters if t == 'train'])\n",
    "    test_idx = [i for t, i in clusters if t == 'test'][0].iloc[:, 0]\n",
    "\n",
    "    prot_train = tf.gather(protein_data, train_idx)\n",
    "    prot_test = tf.gather(protein_data, test_idx)\n",
    "\n",
    "    #smiles_train = tf.gather(smiles_data, train_idx)\n",
    "    #smiles_test = tf.gather(smiles_data, test_idx)\n",
    "\n",
    "    kd_train = tf.gather(kd_values, train_idx)\n",
    "    kd_test = tf.gather(kd_values, test_idx)\n",
    "\n",
    "    FLAGS.optimizer_fn = FLAGS.optimizer_fn[0]\n",
    "\n",
    "    if FLAGS.optimizer_fn[0] == 'radam':\n",
    "        optimizer_fun = tfa.optimizers.RectifiedAdam(learning_rate=float(FLAGS.optimizer_fn[1]),\n",
    "                                                     beta_1=float(FLAGS.optimizer_fn[2]),\n",
    "                                                     beta_2=float(FLAGS.optimizer_fn[3]),\n",
    "                                                     epsilon=float(FLAGS.optimizer_fn[4]),\n",
    "                                                     weight_decay=float(FLAGS.optimizer_fn[5]))\n",
    "    elif FLAGS.optimizer_fn[0] == 'adam':\n",
    "        optimizer_fun = tf.keras.optimizers.Adam(learning_rate=float(FLAGS.optimizer_fn[1]),\n",
    "                                                 beta_1=float(FLAGS.optimizer_fn[2]),\n",
    "                                                 beta_2=float(FLAGS.optimizer_fn[3]),\n",
    "                                                 epsilon=float(FLAGS.optimizer_fn[4]))\n",
    "\n",
    "    elif FLAGS.optimizer_fn[0] == 'adamw':\n",
    "        optimizer_fun = tfa.optimizers.AdamW(learning_rate=float(FLAGS.optimizer_fn[1]),\n",
    "                                             beta_1=float(FLAGS.optimizer_fn[2]),\n",
    "                                             beta_2=float(FLAGS.optimizer_fn[3]), epsilon=float(FLAGS.optimizer_fn[4]),\n",
    "                                             weight_decay=float(FLAGS.optimizer_fn[5]))\n",
    "\n",
    "    dtitr_model = build_dtitr_model(FLAGS, FLAGS.prot_transformer_depth[0], FLAGS.smiles_transformer_depth[0],\n",
    "                                    FLAGS.cross_block_depth[0],\n",
    "                                    FLAGS.prot_transformer_heads[0], FLAGS.smiles_transformer_heads[0],\n",
    "                                    FLAGS.cross_block_heads[0],\n",
    "                                    FLAGS.prot_parameter_sharing[0], FLAGS.prot_dim_k[0],\n",
    "                                    FLAGS.prot_ff_dim[0], FLAGS.smiles_ff_dim[0], FLAGS.d_model[0],\n",
    "                                    FLAGS.dropout_rate[0], FLAGS.dense_atv_fun[0],\n",
    "                                    FLAGS.out_mlp_depth[0], FLAGS.out_mlp_hdim[0], optimizer_fun)\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                          min_delta=0.001, patience=60, mode='min',\n",
    "                                          restore_best_weights=True)\n",
    "\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(filepath=FLAGS.checkpoint_path + '/' + 'dtitr_model_v2' + '/',\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True, save_weights_only=True, mode='min')\n",
    "\n",
    "    dtitr_model.fit(x=[prot_train ], y=kd_train,\n",
    "                    batch_size=FLAGS.batch_dim[0], epochs=FLAGS.num_epochs[0],\n",
    "                    verbose=2, callbacks=[es, mc],\n",
    "                    validation_data=([prot_test], kd_test))\n",
    "\n",
    "    mse, rmse, ci = dtitr_model.evaluate([prot_test], kd_test)\n",
    "\n",
    "    # dtitr_model.save('dtitr_model.h5')\n",
    "\n",
    "    logging(\"Test Fold - \" + (\" MSE = %0.3f, RMSE = %0.3f, CI = %0.3f\" % (mse, rmse, ci)), FLAGS)\n",
    "\n",
    "\n",
    "def run_evaluation_model(FLAGS):\n",
    "    \"\"\"\n",
    "    Run Evaluation function\n",
    "\n",
    "    Args:\n",
    "    - FLAGS: arguments object\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    protein_data, kd_values = dataset_builder(FLAGS.data_path).transform_dataset(FLAGS.bpe_option[0],\n",
    "                                                                                              FLAGS.bpe_option[1],\n",
    "                                                                                              'Sequence',\n",
    "                                                                                              'SMILES',\n",
    "                                                                                              'Kd',\n",
    "                                                                                              FLAGS.protein_bpe_len,\n",
    "                                                                                              FLAGS.protein_len,\n",
    "                                                                                              FLAGS.smiles_bpe_len,\n",
    "                                                                                              FLAGS.smiles_len)\n",
    "\n",
    "    if FLAGS.bpe_option[0] == True:\n",
    "        protein_data = add_reg_token(protein_data, FLAGS.protein_dict_bpe_len)\n",
    "    else:\n",
    "        protein_data = add_reg_token(protein_data, FLAGS.protein_dict_len)\n",
    "\n",
    "    _, _, _, clusters, _, _, _, _ = dataset_builder(FLAGS.data_path).get_data()\n",
    "\n",
    "    test_idx = [i for t, i in clusters if t == 'test'][0].iloc[:, 0]\n",
    "\n",
    "    prot_test = tf.gather(protein_data, test_idx)\n",
    "\n",
    "    smiles_test = tf.gather(smiles_data, test_idx)\n",
    "\n",
    "    kd_test = tf.gather(kd_values, test_idx)\n",
    "\n",
    "    optimizer_fun = tfa.optimizers.RectifiedAdam(learning_rate=1e-04, beta_1=0.9,\n",
    "                                                 beta_2=0.999, epsilon=1e-08,\n",
    "                                                 weight_decay=1e-05)\n",
    "\n",
    "    dtitr_model = build_dtitr_model(FLAGS, 3, 3, 1, 4, 4, 4, '', '', 512, 512, 128, 0.1, 'gelu', 3, [512, 512, 512],\n",
    "                                    optimizer_fun)\n",
    "\n",
    "    dtitr_model.load_weights('../model/dtitr_model/')\n",
    "\n",
    "    metrics = inference_metrics(dtitr_model, [prot_test, kd_test])\n",
    "\n",
    "    logging(metrics, FLAGS)\n",
    "    pred_scatter_plot(kd_test, dtitr_model.predict([prot_test, smiles_test])[:, 0],\n",
    "                      'Davis Dataset: Predictions vs True Values', 'True Values', 'Predictions',\n",
    "                      False, '')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "    FLAGS = argparser()\n",
    "    FLAGS.log_dir = os.getcwd() + '/logs/' + time.strftime(\"%d_%m_%y_%H_%M\", time.gmtime()) + \"/\"\n",
    "    FLAGS.checkpoint_path = os.getcwd() + '/checkpoints/' + time.strftime(\"%d_%m_%y_%H_%M\", time.gmtime()) + \"/\"\n",
    "    FLAGS.data_path = {'data': '../data/davis/dataset/davis_dataset_processed.csv',\n",
    "                       'prot_dic': '../dictionary/davis_prot_dictionary.txt',\n",
    "                       'smiles_dic': '../dictionary/davis_smiles_dictionary.txt',\n",
    "                       'clusters': glob.glob('../data/davis/clusters/*'),\n",
    "                       'prot_bpe': ['../dictionary/protein_codes_uniprot.txt',\n",
    "                                    '../dictionary/subword_units_map_uniprot.csv'],\n",
    "                       'smiles_bpe': ['../dictionary/drug_codes_chembl.txt',\n",
    "                                      '../dictionary/subword_units_map_chembl.csv']}\n",
    "\n",
    "    if not os.path.exists(FLAGS.log_dir):\n",
    "        os.makedirs(FLAGS.log_dir)\n",
    "    if not os.path.exists(FLAGS.checkpoint_path):\n",
    "        os.makedirs(FLAGS.checkpoint_path)\n",
    "\n",
    "    logging(str(FLAGS), FLAGS)\n",
    "\n",
    "    if FLAGS.option == 'Train':\n",
    "        run_train_model(FLAGS)\n",
    "\n",
    "    if FLAGS.option == 'Validation':\n",
    "        run_grid_search(FLAGS)\n",
    "\n",
    "    if FLAGS.option == 'Evaluation':\n",
    "        run_evaluation_model(FLAGS)\n",
    "\n",
    "# training function at each epoch\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "\n",
    " model.train()\n",
    " for batch_idx, data in enumerate(train_loader):\n",
    "   data = data.to(device)\n",
    "   optimizer.zero_grad()\n",
    "   output = model(data)\n",
    "   loss = loss_fn(output, data.y.view(-1, 1))\n",
    "   loss.backward()\n",
    "   optimizer.step()\n",
    "   if batch_idx % LOG_INTERVAL == 0:\n",
    "     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                                                                           batch_idx * len(data.x),\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader),\n",
    "                                                                           loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdz_KEYE0oq8",
    "outputId": "9a42ddd1-989b-4d7d-91d4-2a8b3cb6b38a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/content/drive/MyDrive/GraphDTA-master-2/training.py\", line 16\n",
      "    from transformer-encoder-cross import *\n",
      "                    ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python training.py 0 2 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YA-TTZ19BeV3",
    "outputId": "0891d02a-7ff7-4a7f-9704-4df6489845f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0005\n",
      "Epochs:  2\n",
      "\n",
      "running on  GAT_GATNet_davis\n",
      "Pre-processed data found: data/processed/davis_train.pt, loading ...\n",
      "Pre-processed data found: data/processed/davis_test.pt, loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 25046 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [0/25046 (0%)]\tLoss: 0.107014\n",
      "Train epoch: 1 [329680/25046 (41%)]\tLoss: 0.135768\n",
      "Train epoch: 1 [657720/25046 (82%)]\tLoss: 0.093029\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  1 ; best_mse,best_ci: 0.24873847 0.8773788627588337 GAT_GATNet davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 2 [0/25046 (0%)]\tLoss: 0.072308\n",
      "Train epoch: 2 [331360/25046 (41%)]\tLoss: 0.098862\n",
      "Train epoch: 2 [659080/25046 (82%)]\tLoss: 0.069238\n",
      "Make prediction for 5010 samples...\n",
      "0.2679761 No improvement since epoch  1 ; best_mse,best_ci: 0.24873847 0.8773788627588337 GAT_GATNet davis\n"
     ]
    }
   ],
   "source": [
    "dataset = 'davis'\n",
    "#kiba\n",
    "modeling = [GAT_GATNet]\n",
    "model_st = modeling[0].__name__\n",
    "\n",
    "cuda_name = \"cuda:0\"\n",
    "# if len(sys.argv)>3:\n",
    "#     cuda_name = \"cuda:\" + str(int(sys.argv[3]))\n",
    "# print('cuda_name:', cuda_name)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 512\n",
    "TEST_BATCH_SIZE = 512\n",
    "LR = 0.0005\n",
    "LOG_INTERVAL = 20\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "print('Learning rate: ', LR)\n",
    "print('Epochs: ', NUM_EPOCHS)\n",
    "\n",
    "# Main program: iterate over different datasets\n",
    "# for dataset in dataset:\n",
    "print('\\nrunning on ', model_st + '_' + dataset )\n",
    "processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n",
    "processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n",
    "if ((not os.path.isfile(processed_data_file_train)) or \\\n",
    " (not os.path.isfile(processed_data_file_test))):\n",
    "    print('please run create_data.py to prepare data in pytorch format!')\n",
    "else:\n",
    "    train_data = TestbedDataset(root='data', dataset=dataset+'_train')\n",
    "    test_data = TestbedDataset(root='data', dataset=dataset+'_test')\n",
    "\n",
    "    # make data PyTorch mini-batch processing ready\n",
    "    train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# training the model\n",
    "    device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = modeling\n",
    "    for modeling in [GAT_GATNet]:\n",
    "      model = modeling().to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    best_mse = 0.24929152\n",
    "    best_ci =  0.8811023101087032\n",
    "    best_epoch = 853\n",
    "    model_file_name = 'model_' + model_st + '_' + dataset +  '.model'\n",
    "    result_file_name = 'result_' + model_st + '_' + dataset +  '.csv'\n",
    "    model.load_state_dict(torch.\\\n",
    "    load('/content/drive/MyDrive/GraphDTA-master/model_GAT_GATNet_davis.model',\\\n",
    "         map_location=torch.device('cpu'))\n",
    "    )\n",
    "     #map_location=torch.device('cpu'))) # load the state_dict on CPU\n",
    "    # model.train()\n",
    "\n",
    "    mse_list = []\n",
    "    ci_list = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "      torch.cuda.empty_cache()\n",
    "      train(model, device, train_loader, optimizer, epoch+1)\n",
    "      G,P = predicting(model, device, test_loader)\n",
    "      ret = [rmse(G,P),mse(G,P),pearson(G,P),spearman(G,P),ci(G,P)]\n",
    "\n",
    "\n",
    "      mse_list.append(ret[1])\n",
    "      ci_list.append(ret[-1])\n",
    "      if ret[1]<best_mse:\n",
    "          torch.save(model.state_dict(), \\\n",
    "                '/content/drive/MyDrive/GraphDTA-master/model_GAT_GATNet_davis.model')\n",
    "          with open(result_file_name,'w') as f:\n",
    "              f.write(','.join(map(str,ret)))\n",
    "          best_epoch = epoch+1\n",
    "          best_mse = ret[1]\n",
    "          best_ci = ret[-1]\n",
    "\n",
    "\n",
    "          print('rmse improved at epoch ', best_epoch, '; best_mse,best_ci:',\\\n",
    "                best_mse,best_ci,model_st,dataset)\n",
    "\n",
    "      else:\n",
    "          print(ret[1],'No improvement since epoch ', best_epoch,\\\n",
    "                '; best_mse,best_ci:', best_mse,best_ci,model_st,dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJN2dLKR8CGF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SglIrZyXq5C4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#import dataloader as dataloader\n",
    "# training function at each epoch\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                                            batch_idx * len(data.x),\n",
    "                                            len(train_loader.dataset),\n",
    "                                            100. * batch_idx / len(train_loader),\n",
    "                                            loss.item()))\n",
    "\n",
    "def predicting(model, device, loader):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "    return total_labels.numpy().flatten(),total_preds.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Nf4J2xrJ61o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
